{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.set(font_scale=1.6)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainDataLoad(market=True,news=True):\n",
    "    try:\n",
    "        from kaggle.competitions import twosigmanews\n",
    "\n",
    "        env = twosigmanews.make_env()\n",
    "        (market_df, news_df) = env.get_training_data()\n",
    "\n",
    "        (market_train_df.shape, news_train_df.shape)\n",
    "    except:\n",
    "        print('failed to load data from kaggle, loading data from local directory.')\n",
    "        if(market):\n",
    "            market_df=pd.read_csv('./sampleData/market_train.csv')\n",
    "        if(news):\n",
    "            news_df=pd.read_csv('./sampleData/news_train.csv')\n",
    "    print('Train data loaded!')\n",
    "    return (market_df,news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeCut(df,time, replace=True):\n",
    "    '''\n",
    "    df: dataFrame with attribute time in datatime64 format\n",
    "    time: a time in string\n",
    "    return df slice cutting off the time before the time provided\n",
    "    '''\n",
    "    df.time=pd.to_datetime(df.time)\n",
    "    time=pd.Timestamp(time)\n",
    "    df_slice = df[df.time>time]\n",
    "    if replace:\n",
    "        df=df_slice\n",
    "    return df_slice\n",
    "\n",
    "def formatCodeSet(df,field):\n",
    "    '''\n",
    "    df:dataframe\n",
    "    field:field name of the code in the form string in set format\n",
    "    return the field formatted into array\n",
    "    '''\n",
    "    return df[field].str.findall(f\"'([\\w\\./]+)'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to load data from kaggle, loading data from local directory.\n",
      "Train data loaded!\n"
     ]
    }
   ],
   "source": [
    "#Load Data\n",
    "(market_train_df,news_train_df)=trainDataLoad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Depends on the need, cut the data into smaller size for dev testing to save resources\n",
    "\n",
    "time='2012-12-31'\n",
    "# its best to get the data with a time cut\n",
    "if time:\n",
    "    market_train_df = timeCut(market_train_df,time)\n",
    "    news_train_df = timeCut(news_train_df,time)\n",
    "    \n",
    "news_train_df['subjects'] = formatCodeSet(news_train_df,'subjects')\n",
    "news_train_df['audiences'] = formatCodeSet(news_train_df,'audiences')\n",
    "news_train_df['assetCodes'] = formatCodeSet(news_train_df,'assetCodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - News data features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The news dataset already included many engineered features for prediction. However it would be nice to further explore the headline features with different kind of embeddings. In order to include the data in the news headlines, it would be useful to apply document embedding so that the model can \"understand\" the document contents and improve its prediction accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "def tokeniser_wrapper(tokeniser):\n",
    "    '''\n",
    "    A tokeniser wrapper proxy to handle exceptions\n",
    "    '''\n",
    "    def wrapped(tokeniser,text):\n",
    "        try:\n",
    "            return tokeniser(text)\n",
    "        except:\n",
    "            print('Failed tokenisation on input:',text)\n",
    "            return []\n",
    "    return lambda text:wrapped(tokeniser,text)\n",
    "    \n",
    "    \n",
    "def getTokens(tokeniser, textCol, iterator=True):\n",
    "    '''\n",
    "    Take in a text column and then return an array of tokenised entries\n",
    "    '''\n",
    "    if not iterator:\n",
    "        return list(map(tokeniser,textCol.as_matrix()))\n",
    "    return map(tokeniser,textCol.as_matrix())\n",
    "\n",
    "#Tokenisers\n",
    "tknsr=tokeniser_wrapper(nltk.word_tokenize)\n",
    "tknsr_noPunc = tokeniser_wrapper(nltk.tokenize.RegexpTokenizer('\\w+').tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n",
      "Failed tokenisation on input: nan\n"
     ]
    }
   ],
   "source": [
    "news_train_df['headline_tokens']=list(map(tknsr,news_train_df['headline'].as_matrix()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "Embeddings mean turning certain categorical/text data into meaningful vectors that can be \"ingested\" by a machine learning model. Word embeddings is a very common technique used in NLP. By turning words into vectors, we can further compose a meaning representation vectors for each document.\n",
    "\n",
    "There are different models that can further encode word embeddings into a document embeddings which would be discussed later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document embeddings\n",
    "\n",
    "While some other models creates embeddings for words, there are also other models that can directly create embeddings for documents of various length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train for embeddings, we would first need to tokenise the headline sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings - 1. Word2Vec\n",
    "Original paper: https://arxiv.org/pdf/1310.4546.pdf\n",
    "\n",
    "Blog referece: https://www.knime.com/blog/word-embedding-word2vec-explained\n",
    "\n",
    "Further formalisation: https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf\n",
    "\n",
    "Pretrained embeddings:\n",
    "\n",
    "1. google news: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "2. freebase entity: https://docs.google.com/file/d/0B7XkCwpI5KDYaDBDQm1tZGNDRHc/edit\n",
    "\n",
    "\n",
    "Word2Vec is one of the most famous embedding for words first [published by Google in 2013](https://arxiv.org/pdf/1310.4546.pdf). It combines the CBOW and the Skip-gram structure to form an encoder. The representation learnt is an embedded vector which encode the coocurrences probability between words and context. The model applied a numerous different technique to simplify the calculations such as `negative sampling` and `hierachical softmax` which are as well applied in other embedding models developed afterwards.\n",
    "\n",
    "It can be viewed as an auto-encoder model for context-word in terms of deep-learning or [a factorisation(approximation) of the context-words pointwise-mutual information matrix](https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf). \n",
    "\n",
    "The understanding of the latter would help combining the model with other mathematical/statistical models to generate quantified results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom embedding - Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_w2vCustom = Word2Vec(sentences=news_train_df['headline_tokens'], size=100, window=5, min_count=5, workers=4, sg=0)\n",
    "#model_w2vCustom.save('./sampleData/word2vecCustom.model')\n",
    "model_w2vCustom=Word2Vec.load('./sampleData/word2vecCustom.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrained-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "fileDir='C:/Users/CK/Downloads/'\n",
    "freebase_entity='knowledge-vectors-skipgram1000.bin'\n",
    "google_news='GoogleNews-vectors-negative300.bin'\n",
    "model_w2vGoogleNews = gensim.models.KeyedVectors.load_word2vec_format(fileDir+google_news, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#news_df=pd.read_csv('./sampleData/news_train.csv', encoding = \"ISO-8859-1\")\n",
    "#Create the model\n",
    "model_w2vGoogleNewsPlus=gensim.models.Word2Vec(size=300)\n",
    "#Build the new vocabs\n",
    "model_w2vGoogleNewsPlus.build_vocab(news_df['headline_tokens'])\n",
    "#Read in the pretrained vectors\n",
    "model_w2vGoogleNewsPlus.intersect_word2vec_format(fileDir+google_news, binary=True)\n",
    "model_w2vGoogleNewsPlus.train(news_df['headline_tokens'],total_examples=model_NewsPlus.corpus_count,epochs=10)\n",
    "model_w2vGoogleNewsPlus.save('./sampleData/word2vecTransfer.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings - 2. Fastext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original paper: https://arxiv.org/pdf/1607.04606.pdf\n",
    "\n",
    "Blog reference: https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c\n",
    "\n",
    "Pre-trained embeddings:\n",
    "    1. https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md\n",
    "    \n",
    "Fasttext is very similar to word2vec but instead of simply using the whole world, it splits words into subwords as inputs and sum the vectors as the embedding of the word. Such method can generalise the meaning of the words to unseen words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom embedding - Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_FTCustom = FastText(sentences=news_train_df['headline_tokens'], size=100, window=5, min_count=5, workers=4, sg=0)\n",
    "model_FTCustom.save('./sampleData/FastTextCustom.model')\n",
    "#model_FTCustom=FastText.load('./sampleData/word2vecCustom.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrained-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model_FTgoogleNews = gensim.models.FastText.load_FastText_format(fileDir+google_news, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#news_train_df=pd.read_csv('./sampleData/news_train.csv', encoding = \"ISO-8859-1\")\n",
    "model_FTgoogleNewsPlus=gensim.models.FastText(size=300)\n",
    "model_FTgoogleNewsPlus.intersect_FastText_format(fileDir+google_news, binary=True)\n",
    "model_FTgoogleNewsPlus.build_vocab(news_train_df['headline_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1442456173, 5314372690)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_FTgoogleNewsPlus.train(news_train_df['headline_tokens'],total_examples=model_NewsPlus.corpus_count,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document embedding - 1. Doc2Vec\n",
    "\n",
    "Original paper: https://arxiv.org/pdf/1405.4053.pdf\n",
    "\n",
    "blog summary: https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e\n",
    "\n",
    "Doc2vec is similar to word2vec. Both of them use the same techniques and encode-decoding model except that now it is embedding the document vector as one of the input vectors for CBOW and using only the document vector to reconstruct the context in skip-gram. The goal is to train the model to learn how to represent a document vector based on the content. \n",
    "\n",
    "As it is to learn the document vector summarised, the population distribution of the documents is also important. Meaning that properly control the source of the training document can better encode the meaning of the documents into vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom embedding - Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from ast import literal_eval\n",
    "documents = [TaggedDocument(literal_eval(content), [i]) for i,content in news_train_df['headline_tokens'].iteritems()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_d2vCustom = Doc2Vec(documents=documents,vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_d2vCustom.save('./sampleData/model_d2vCustom100.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
