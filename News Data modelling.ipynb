{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Data analysis and engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The news have 3 locative dimensions\n",
    "- Provider\n",
    "- Subjects(related assets)\n",
    "- Audiences\n",
    "\n",
    "Provider: For provider, over 75% of the news are provided by RTRS and the rest of the top 5 are 5-7% . Under this circumstances, it may make sense to group news providers into channels while top 5 have their own channels and the rest are grouped together as \"others\".\n",
    "\n",
    "Subjects: There are numerous assets being mentioned in the datasets. The idea is to use asset embedding to represent assets. The embedding should have linear similarity property(i.e. we can tell how similar two assets are by calculating their dot product normalised by the multiplication of their lengths.)\n",
    "\n",
    "Suppose we originally have news data features $F^n=(F_1^n,...,F_M^n)$ a vector of dimension $M$ for news $n$ and $S$ number of subject assets with respective assets embeddings $A_1^n,...,A_S^n$.  For an asset $A$, the relevancy of the news signal with respect to the asset $A$ is \n",
    "\n",
    "$$ A \\cdot \\overline{A_n}$$, where $\\overline{A_n}= \\frac{(\\Sigma_i A_i^n)}{S}$.\n",
    "\n",
    "The intuition is that the assets similarity will be able to direct the news signals to relevant assents with the same mechanism as the famous attention mechanism.\n",
    "\n",
    "By doing so, we have made a few assumptions:\n",
    "1. The attention mechanism is effective. (If it is not effective, summing over a lot of weak signals would confuse the model). Ideally, we would want to have $A \\cdot B$ close to either 1 or 0 for any two assets $A$ and $B$. \n",
    "2. news features $F^n$ has independent entries so summing two news feature vectors will not break the association nature of the featues. For that, we may need to implement non-linear transformation of the features to turn them into independent entries.\n",
    "3. Each news is assets specific as taking average would destroy the distribution information. i.e. it is related to certain kind of assets with high similarity.\n",
    "\n",
    "Within a unit timeframe with $N$ news, the signal directing towards a particular asset $A$ can be estimated as,\n",
    "\n",
    "$$\\Sigma_n^N (A \\cdot \\overline{A_n}  \\times F^n)$$.\n",
    "\n",
    "However, by doing so, the signals may be maginified when there are multiple articles talking about the same asset within the timeframe. To address this problem, There could be two approaches, \n",
    "\n",
    "1. Apply another mechanism that can summarise the news asset-wise with some kind of recurrent structure or attention weightings.\n",
    "2. Shrinking the news data partition(i.e. Shorten the unit timeframe and spread the news channels further) such that the chance of having duplicated news within a short period will be rare and relie on the convolution network to take care of the summarisation of news over a longer period of time. To test such approach, we would need to creat partition and do a partition count on the assets. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "96859b4186cf57bb87a921a4b3d3074ab914a743"
   },
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is highly replicative. The same news would be issued for different audience at different time. Also, multiple news could have mentioned about an assets within the same unit timeframe. There are properties regarding the novelty of the news and the volumn issued. Also, news could be replicated with only a small difference in word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.set(font_scale=1.6)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def trainDataLoad(env=False,market=True,news=True):\n",
    "    try:\n",
    "        from kaggle.competitions import twosigmanews\n",
    "\n",
    "        if(not env):\n",
    "            env = twosigmanews.make_env()\n",
    "        (market_df, news_df) = env.get_training_data()\n",
    "\n",
    "        print('Data fetched from kaggle with {} rows of market data and {} rows of news data'.format(market_df.shape, news_df.shape))\n",
    "    except:\n",
    "        print('failed to load data from kaggle, loading data from local directory.')\n",
    "        if(market):\n",
    "            market_df=pd.read_csv('./sampleData/market_train.csv')\n",
    "        if(news):\n",
    "            news_df=pd.read_csv('./sampleData/news_train.csv')\n",
    "        print('Train data loaded!')\n",
    "    if(market & (!news)):\n",
    "        return market_df\n",
    "    if(news & (!market)):\n",
    "        return news_df\n",
    "    return (market_df,news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91a60f91518ae87c7afd83737db43519fbc762bd"
   },
   "outputs": [],
   "source": [
    "def timeCut(df,timeStart,timeEnd=pd.Timestamp.now(), replace=True):\n",
    "    '''\n",
    "    df: dataFrame with attribute time in datatime64 format\n",
    "    time: a time in string\n",
    "    return df slice cutting off the time before the time provided\n",
    "    '''\n",
    "    df.time=pd.to_datetime(df.time)\n",
    "    timeStart=pd.Timestamp(timeStart)\n",
    "    timeEnd=pd.Timestamp(timeEnd)\n",
    "    df_slice = df[(df.time>timeStart) & (df.time<timeEnd)]\n",
    "    if replace:\n",
    "        df=df_slice\n",
    "    return df_slice\n",
    "\n",
    "def formatCodeSet(df,field):\n",
    "    '''\n",
    "    df:dataframe\n",
    "    field:field name of the code in the form string in set format\n",
    "    return the field formatted into array\n",
    "    '''\n",
    "    return df[field].str.findall(f\"'([\\w\\./]+)'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e214f1e0769e988d6c1cb668b3a618560f04a42"
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "(market_train_df,news_train_df)=trainDataLoad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e53b307b3a878c1e2dfa5997087948ef36bf6d01"
   },
   "outputs": [],
   "source": [
    "# Cut it into a slice for exploratory purpose\n",
    "df_2007=timeCut(news_train_df,'2007-1-1 22:00:00+00:00','2007-12-31 22:00:00+00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce3b288626c07da86555edfa2d59ca00eee71eed"
   },
   "outputs": [],
   "source": [
    "# Adding time partition for cuts\n",
    "df_2007.loc[:,'year']=df_2007.time.apply(lambda x:x.year)\n",
    "df_2007.loc[:,'month']=df_2007.time.apply(lambda x:x.month)\n",
    "df_2007.loc[:,'day']=df_2007.time.apply(lambda x:x.day)\n",
    "df_2007.loc[:,'hour']=df_2007.time.apply(lambda x:x.hour)\n",
    "df_2007.loc[:,'minute']=df_2007.time.apply(lambda x:x.minute)\n",
    "df_2007.loc[:,'second']=df_2007.time.apply(lambda x:x.second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c35c4a931884cddab4e38431901454287b508148"
   },
   "outputs": [],
   "source": [
    "df_2007.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "_uuid": "d81300d03ef9ab13dcc42284a64730ae84320eaf"
   },
   "outputs": [],
   "source": [
    "time_partition=['month','day','hour']\n",
    "features1=['headline', 'urgency', 'provider', 'subjects', 'audiences', 'bodySize', 'companyCount', 'marketCommentary', 'sentenceCount', 'wordCount', 'assetCodes', 'assetName', 'firstMentionSentence', 'relevance', 'sentimentClass', 'sentimentNegative', 'sentimentNeutral', 'sentimentPositive', 'sentimentWordCount', 'noveltyCount12H', 'noveltyCount24H', 'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H', 'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D', 'volumeCounts7D']\n",
    "features2=['provider','assetCodes','audiences']\n",
    "partition=time_partition+features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "_uuid": "73b213a4335442172537869ea7ba3fb49111438a",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     417420\n",
       "2      56638\n",
       "3      14875\n",
       "4       7438\n",
       "5       4177\n",
       "6       2630\n",
       "7       1834\n",
       "8       1233\n",
       "9        846\n",
       "10       628\n",
       "11       432\n",
       "12       254\n",
       "13       209\n",
       "14       140\n",
       "16        80\n",
       "15        77\n",
       "17        51\n",
       "18        31\n",
       "20        19\n",
       "21        18\n",
       "19        18\n",
       "22        16\n",
       "24        12\n",
       "23        10\n",
       "25         8\n",
       "26         8\n",
       "28         5\n",
       "27         4\n",
       "31         3\n",
       "32         1\n",
       "34         1\n",
       "33         1\n",
       "39         1\n",
       "30         1\n",
       "29         1\n",
       "57         1\n",
       "Name: sourceTimestamp, dtype: int64"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_2007=df_2007[partition+['sourceTimestamp']].groupby(partition)['sourceTimestamp'].count().reset_index()\n",
    "partition_2007['sourceTimestamp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a0cfb1cbe0deb065fc6b0bcecb08afa448abfb8"
   },
   "outputs": [],
   "source": [
    "partition_2007[partition_2007['sourceTimestamp']>30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9aff2bbe106af74e40db5aa04752fda95fac7286"
   },
   "outputs": [],
   "source": [
    "df_2007[(df_2007.month == 2) & (df_2007.day==20) & (df_2007.hour==21) & (df_2007.assetCodes==\t\"{'HPQ.DE', 'HPQ.N'}\") ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "818f51df1983da518d56f1f6c394086706f28581"
   },
   "outputs": [],
   "source": [
    "df_2007[df_2007.time=='2007-01-17 12:00:18+00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ed1bcb206e39326561c68992480a1dd4060c390"
   },
   "outputs": [],
   "source": [
    "df_2007_u=df_2007.drop_duplicates(subset=features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eaa140adcf2576ef34977ecb5d3077c59e6c1c41"
   },
   "outputs": [],
   "source": [
    "partition_2007=df_2007_u[partition+['sourceTimestamp']].groupby(partition)['sourceTimestamp'].count().reset_index()\n",
    "partition_2007['sourceTimestamp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b3aba6e8b41a543de1ad6c1d38048796f0a07c4"
   },
   "outputs": [],
   "source": [
    "time_partition=['time']\n",
    "features=['provider','assetCodes','audiences']\n",
    "partition=time_partition+features\n",
    "partition_2007=df_2007_u[partition+['sourceTimestamp']].groupby(partition)['sourceTimestamp'].count().reset_index()\n",
    "partition_2007['sourceTimestamp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6cd9570d16b064238daaebdc37b80ba4ea4b0ef3"
   },
   "outputs": [],
   "source": [
    "partition_2007[partition_2007['sourceTimestamp']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3eafe98f201815b3c6245e000fe94038fb129b00"
   },
   "outputs": [],
   "source": [
    "df_2007_u[df_2007_u.time=='2007-01-02 20:04:09+00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c2962420343814bf0e50a48771b0008e3544cb21"
   },
   "outputs": [],
   "source": [
    "partition_2007=df_2007[partition+['sourceTimestamp']].groupby(partition)['sourceTimestamp'].count().reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline\n",
    "\n",
    "1. Determine news features and unit timeframe.\n",
    "2. Non-linear transformation of news features, hoping that will transform the data into independent entries.\n",
    "3. Recurrent summary of news within the same channel.\n",
    "    - Use an attention key to do weighted average of features.\n",
    "    - Use recurrent networks to filter important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
